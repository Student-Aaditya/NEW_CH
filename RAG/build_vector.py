# FILE: RAG/build_vectors.py

import os
import json
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

# =====================================================
# PATHS (Auto Detect)
# =====================================================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")  # contains all chunk files
META_OUTPUT = os.path.join(BASE_DIR, "metadata_cleaned.json")
VEC_OUTPUT = os.path.join(BASE_DIR, "vectors.npy")
FAISS_OUTPUT = os.path.join(BASE_DIR, "vector_store.faiss")

# =====================================================
# LOAD EMBEDDING MODEL
# =====================================================
MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
print(f"üìå Loading model: {MODEL_NAME}")
model = SentenceTransformer(MODEL_NAME)

# =====================================================
# READ ALL CHUNK FILES
# =====================================================
def load_all_chunks():
    all_chunks = []
    print("\nüìÅ Reading chunk files from:", DATA_DIR)

    for file in os.listdir(DATA_DIR):
        if file.endswith(".json"):
            path = os.path.join(DATA_DIR, file)
            print(f"   ‚ûù Found: {file}")

            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
                all_chunks.extend(data)

    print(f"\nüìå Total Chunks Loaded: {len(all_chunks)}")
    return all_chunks


# =====================================================
# BUILD VECTORS & INDEX
# =====================================================
def build_vector_store():
    chunks = load_all_chunks()

    if not chunks:
        print("‚ùå No JSON chunk files found in /data. Add chunk files first!")
        return

    texts = [item["answer"] for item in chunks]

    print("\n‚öô Generating Embeddings (this may take time)...")
    vectors = model.encode(texts)
    vectors = np.array(vectors).astype("float32")

    print("üíæ Saving metadata & vectors...")
    np.save(VEC_OUTPUT, vectors)

    with open(META_OUTPUT, "w", encoding="utf-8") as f:
        json.dump(chunks, f, indent=4, ensure_ascii=False)

    # FAISS index
    dim = vectors.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(vectors)
    faiss.write_index(index, FAISS_OUTPUT)

    print("\nüéâ VECTOR STORE BUILT SUCCESSFULLY!")
    print("üìç Files Generated:")
    print(f"   - {FAISS_OUTPUT}")
    print(f"   - {VEC_OUTPUT}")
    print(f"   - {META_OUTPUT}")
    print("\nüöÄ RAG is ready to use!")


# =====================================================
# EXECUTION
# =====================================================
if __name__ == "__main__":
    print("üöÄ STARTING VECTOR BUILD PROCESS...")
    build_vector_store()
